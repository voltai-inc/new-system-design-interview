# System Design Interview: Agentic Document Q&A Pipeline

## Overview

**Position Level:** Mid-level (3-5 years experience)
**Format:** 45-60 minute live whiteboard/virtual session
**Style:** Conversational with optional diagrams

**Core Skills Evaluated:**
1. Context window management - handling large docs that don't fit in a single LLM call
2. Multi-step reasoning orchestration - chaining agent calls for multi-hop queries
3. Accuracy vs latency trade-offs - when to add verification passes vs return faster

---

## Problem Statement

*Read this to the candidate at the start:*

> **Problem: Document Q&A Subagent**
>
> You're building a subagent for a larger AI system. The main agent receives user questions about Infineon microcontrollers and delegates to your subagent with:
> - A **query** (well-formatted, no typos - generated by the main agent)
> - A **list of PDF documents** to search (1-7 documents, each 5-500 pages)
>
> Your subagent must:
> 1. Find the relevant location(s) in those documents
> 2. Return an answer with citations
>
> **Constraints:**
> - No pre-built embeddings or vector indexes - documents are provided at query time
> - Latency budget: 30 seconds to 5 minutes
> - Must handle queries that require information from multiple locations
>
> **Example:** *"What software timeout should I budget for a full device flash erase on the TC36x?"*
> - Answer requires combining info from the datasheet (base timing: 5.0s) AND the errata sheet (revised timing: 5.16s)
>
> *Design a system to solve this.*

---

## Progressive Complexity Layers

Start with Layer 1. Progress based on candidate performance and time remaining.

---

### Layer 1: Single Document, Simple Lookup
**Time:** 5-10 minutes

**Prompt to candidate:**
> "Let's start simple - one 50-page PDF, query is 'What is the max QSPI speed on TC364?'"

**What you're evaluating:**
- Do they recognize the doc might not fit in context window?
- Do they propose chunking or page-by-page scanning?
- Basic orchestration: query → search → extract → respond

**Probe questions:**
- "How big is a typical context window? How many pages fit?"
- "Walk me through what happens step by step when a query comes in."

**Green flags:**
- Asks about context window limits unprompted
- Proposes iterative scanning or chunking
- Considers table of contents / section headers as navigation aid

**Red flags:**
- Assumes entire PDF fits in one call
- Jumps straight to "just send it to GPT-4"
- No consideration of document structure

---

### Layer 2: Large Document, Context Window Pressure
**Time:** 10-15 minutes

**Prompt to candidate:**
> "Now the PDF is 500 pages. How does your design change?"

**What you're evaluating:**
- Chunking strategy (by page? by section? overlapping windows?)
- Parallel vs sequential processing
- How to know when you've found "enough"

**Probe questions:**
- "How do you chunk the document? Fixed pages or semantic boundaries?"
- "Do you process chunks in parallel or sequence? Why?"
- "You found a relevant section on page 50. Do you keep searching or stop?"
- "What if the answer spans a chunk boundary?"

**Green flags:**
- Discusses chunk overlap to handle boundary issues
- Proposes hierarchical search (TOC/headers first, then drill into sections)
- Considers parallel chunk processing with aggregation
- Has a stopping condition strategy (confidence threshold, exhaustive search, etc.)

**Red flags:**
- Linear scan with no optimization
- No discussion of when to stop searching
- Ignores chunk boundary problem
- No awareness of latency implications of sequential processing

**Expected insight:** Hierarchical approach - use TOC/section headers as a "map" to identify candidate sections, then deep-dive only into promising areas.

---

### Layer 3: Multi-Document Cross-Reference
**Time:** 10-15 minutes

**Prompt to candidate:**
> "The query is 'What timeout for flash erase on TC36x?' The answer requires both the datasheet AND the errata sheet. The errata overrides the datasheet."

**What you're evaluating:**
- How to search multiple documents (parallel? sequential?)
- How to reconcile conflicting information
- Citation tracking across sources

**Probe questions:**
- "What if the datasheet says 5.0s but errata says 5.16s - how does your system know which to trust?"
- "How do you know you need the errata at all? What if you only searched the datasheet?"
- "Do you search documents in parallel or sequence?"
- "How do you track which fact came from which document?"

**Green flags:**
- Proposes document-type awareness (errata > datasheet > user manual hierarchy)
- Suggests searching all provided documents, not just until first answer found
- Has a reconciliation/synthesis step after gathering evidence
- Tracks provenance (source document, page number) throughout pipeline

**Red flags:**
- Treats all sources as equal authority
- Stops at first answer found without checking other docs
- No strategy for conflict resolution
- Loses track of which info came from where

**Expected insight:** Need a "synthesis agent" that receives evidence from multiple sources and applies domain rules (errata overrides datasheet) to produce final answer.

---

### Layer 4: Complex Query Types
**Time:** 10-15 minutes

**Prompt to candidate:**
> "Now the query is 'How many GPIOs are available on the TC364?' The answer requires counting across 12 different tables scattered throughout the datasheet."

**What you're evaluating:**
- Query classification (lookup vs. aggregation vs. multi-hop)
- Orchestration for aggregation tasks
- Accuracy vs latency trade-off (do you verify the count?)

**Probe questions:**
- "How does your system know this is a counting problem, not a simple lookup?"
- "You found 8 tables with GPIO info. How confident are you that you found all of them?"
- "Would you do a verification pass? What's the cost in latency?"
- "What if two tables list the same GPIO port - how do you avoid double-counting?"

**Green flags:**
- Proposes query analysis/classification step upfront
- Different strategies for different query types
- Discusses confidence thresholds and completeness guarantees
- Considers verification pass for high-stakes aggregation queries
- Aware of deduplication challenges

**Red flags:**
- Same approach for all query types regardless of complexity
- No awareness that counting queries need completeness guarantees
- No consideration of verification vs latency trade-off
- Assumes first pass is always correct

**Expected insight:** Aggregation queries need exhaustive search + deduplication + optional verification pass. The 30s-5min latency budget exists precisely to allow for this.

---

## Bonus Layer (If Time Permits)

### Layer 5: Figure and Graph Reading
**Time:** 5-10 minutes

**Prompt to candidate:**
> "The query is 'Do all I/O pins include internal clamping diodes?' The answer is in Figure 94 - a circuit diagram - not in text."

**What you're evaluating:**
- Awareness that PDFs contain non-text content
- Multi-modal considerations
- Graceful degradation when text search fails

**Green flags:**
- Acknowledges need for vision/multi-modal capabilities
- Proposes fallback: search for figure captions/references in text
- Considers OCR for diagram labels

**Red flags:**
- Assumes all answers are in text
- No awareness of multi-modal requirements

---

## Evaluation Rubric

### Strong Hire Signals
- Unprompted awareness of context window limits
- Proposes hierarchical search strategy without hints
- Recognizes need for different strategies for different query types
- Discusses accuracy vs latency trade-offs with nuance
- Considers failure modes (what if search misses relevant section?)
- Asks good clarifying questions about document structure, query distribution

### Hire Signals
- Reaches good solutions with minimal hints
- Solid understanding of chunking and parallel processing
- Recognizes multi-doc reconciliation challenge when prompted
- Can articulate trade-offs when asked

### No Hire Signals
- Cannot move past "send everything to GPT-4"
- No awareness of context window limitations
- Cannot adapt design when complexity increases
- No consideration of accuracy vs latency
- Defensive when assumptions are challenged

---

## Sample Candidate Solutions (For Interviewer Reference)

### Basic Solution (Acceptable for Layer 1-2)

```
┌─────────────────────────────────────────────────────────┐
│                      Query + Docs                        │
└─────────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────┐
│                    Chunking Agent                        │
│         Split each doc into N-page chunks               │
└─────────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────┐
│              Parallel Search (per chunk)                 │
│     "Does this chunk contain info about {query}?"       │
└─────────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────┐
│                   Answer Extraction                      │
│      Extract answer from relevant chunks                │
└─────────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────┐
│                     Final Answer                         │
└─────────────────────────────────────────────────────────┘
```

### Advanced Solution (Expected for Layer 3-4)

```
┌─────────────────────────────────────────────────────────┐
│                      Query + Docs                        │
└─────────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────┐
│                  Query Classifier                        │
│    Determine: lookup | multi-hop | aggregation          │
└─────────────────────────────────────────────────────────┘
                            │
            ┌───────────────┼───────────────┐
            ▼               ▼               ▼
       [Lookup]       [Multi-hop]     [Aggregation]
            │               │               │
            ▼               ▼               ▼
┌─────────────────────────────────────────────────────────┐
│              Hierarchical Document Search                │
│                                                          │
│   1. Extract TOC / Section Headers (fast scan)          │
│   2. Identify candidate sections                        │
│   3. Deep-dive into promising sections only             │
│   4. Track provenance (doc, page, section)              │
└─────────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────┐
│               Evidence Collection                        │
│     Parallel search across all provided documents       │
│     Maintain source attribution for each fact           │
└─────────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────┐
│               Synthesis Agent                            │
│                                                          │
│   - Reconcile conflicts (errata > datasheet)            │
│   - Aggregate counts (with deduplication)               │
│   - Apply domain rules                                  │
│   - Assess confidence                                   │
└─────────────────────────────────────────────────────────┘
                            │
                   ┌────────┴────────┐
                   │  Low Confidence │
                   └────────┬────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────┐
│            Verification Pass (Optional)                  │
│                                                          │
│   - Re-search with refined queries                      │
│   - Check for missed sections                           │
│   - Validate counts                                     │
│   - Cost: +30-60s latency                               │
└─────────────────────────────────────────────────────────┘
                            │
                            ▼
┌─────────────────────────────────────────────────────────┐
│                    Final Answer                          │
│         Answer + Citations + Confidence Score           │
└─────────────────────────────────────────────────────────┘
```

---

## Key Concepts Candidates Should Discover

1. **Context window is the fundamental constraint** - 500 pages ≠ one API call

2. **Hierarchical search beats linear scan** - Use document structure (TOC, headers) as a map

3. **Query type determines strategy** - Lookup, multi-hop, and aggregation need different approaches

4. **Multi-doc requires reconciliation** - Not just "search more docs" but "synthesize with rules"

5. **Accuracy vs latency is a spectrum** - The 5-minute budget exists for verification passes

6. **Provenance tracking is essential** - Can't cite what you can't trace

---

## Common Candidate Mistakes

1. **Underestimating document size** - "Just send it to GPT-4" without considering context limits

2. **One-size-fits-all approach** - Same strategy for simple lookup vs. counting across 12 tables

3. **Stopping too early** - Finding one answer without checking if errata contradicts it

4. **Ignoring confidence** - Treating all extractions as equally reliable

5. **No provenance** - Generating answers without tracking where information came from

6. **Over-engineering early** - Building complex systems before understanding the basic flow

---

## Interview Flow Checklist

- [ ] Read problem statement
- [ ] Let candidate ask clarifying questions (2-3 min)
- [ ] Layer 1: Simple single-doc lookup (5-10 min)
- [ ] Layer 2: Large document pressure (10-15 min)
- [ ] Layer 3: Multi-doc cross-reference (10-15 min)
- [ ] Layer 4: Complex query types (10-15 min)
- [ ] (Optional) Layer 5: Figure reading
- [ ] Wrap-up: "Any questions for me?" (2-3 min)

---

## Reference: Sample Questions from Domain

These are real query types from Infineon microcontroller documentation:

| Query Type | Example | Complexity |
|------------|---------|------------|
| Simple lookup | "What is the max QSPI speed on TC364?" | Single doc, single location |
| Multi-hop | "What timeout for flash erase on TC36x?" | Datasheet + errata cross-reference |
| Aggregation | "How many GPIOs on TC364?" | Count across 12+ tables |
| Figure reading | "Do I/O pins have clamping diodes?" | Answer in circuit diagram |
| Contradiction resolution | "Can I rely on backup clock trimming for LIN timing?" | Datasheet footnote contradicted by errata |

---

## Notes for Interviewer

- **Adjust pacing** based on candidate speed. Strong candidates may reach Layer 4 in 30 minutes.
- **Hints are okay** for mid-level. The goal is to see how they respond to new information.
- **Probe for trade-offs** rather than "right answers." There are multiple valid approaches.
- **Watch for over-engineering.** YAGNI applies - simple solutions that work beat complex solutions that might work.
- **Document-type hierarchy** (errata > datasheet > user manual) is domain knowledge - it's okay to tell them this when they reach Layer 3.
